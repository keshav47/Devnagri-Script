{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras import layers\n",
    "from keras.layers import Input,Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D\n",
    "from keras.layers import AveragePooling2D, MaxPooling2D, Dropout, GlobalMaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.models import Model\n",
    "from keras import regularizers, optimizers\n",
    "from keras.preprocessing import image\n",
    "from keras.utils import layer_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.utils import plot_model\n",
    "from keras.initializers import glorot_uniform\n",
    "import keras.backend as K\n",
    "K.set_image_data_format('channels_last')\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "from keras.models import Sequential\n",
    "%matplotlib inline\n",
    "import cv2\n",
    "import os\n",
    "import scipy\n",
    "from scipy import ndimage\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing Train And Test Set Data Values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_from_folder(folder):    #Function is used to convert all the images in train and test set into \n",
    "    images = []                         #arrays of values of pixels of the corresponding image\n",
    "    for filename in os.listdir(folder):\n",
    "        img = cv2.imread(os.path.join(folder,filename))\n",
    "        if img is not None:\n",
    "            images.append(img)\n",
    "    return images\n",
    "final_images=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images=[]         # Variable X_train contains the array of pixel values of all the images in Training Set\n",
    "for l in range(1,37):\n",
    "    images=load_images_from_folder(\"DevanagariHandwrittenCharacterDataset\\Train\\character_\"+ str(l))\n",
    "    train_images=train_images+images\n",
    "    \n",
    "for l in range(0,10):\n",
    "    images=load_images_from_folder(\"DevanagariHandwrittenCharacterDataset\\Train\\digit_\"+ str(l))\n",
    "    train_images=train_images+images    \n",
    "    \n",
    "X_train=train_images    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(78200, 1)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=[]                   # corressponding labels to the different classes of images in Training Set are saved in train_label (0-45)\n",
    "for i in range(0,46):\n",
    "    for j in range(0,1700):\n",
    "        y.append(i)\n",
    "Y=np.array(y)        \n",
    "train_label=np.reshape(Y,(78200,1))\n",
    "np.shape(train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images=[]          # Variable X_test contains the array of pixel values of all the images in Test Set\n",
    "for l in range(1,37):\n",
    "    images=load_images_from_folder(\"DevanagariHandwrittenCharacterDataset\\Test\\character_\"+ str(l))\n",
    "    test_images=test_images+images\n",
    "\n",
    "for l in range(0,10):\n",
    "    images=load_images_from_folder(\"DevanagariHandwrittenCharacterDataset\\Test\\digit_\"+ str(l))\n",
    "    test_images=test_images+images    \n",
    "\n",
    "X_test=test_images     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13800, 1)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=[]                  # corressponding labels to the different classes of images in Test Set are saved in test_labels (0-45)    \n",
    "for i in range(0,46):\n",
    "    for j in range(0,300):\n",
    "        y.append(i)\n",
    "Y=np.array(y)        \n",
    "test_label=np.reshape(Y,(13800,1))\n",
    "np.shape(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_matrix(labels,C):       # This Function converts the labels into a One Hot Matrix i.e for label such as a=[0,1,2]\n",
    "    C=tf.constant(C)                # the one hot matrix conversion of a would be = [[1,0,0],[0,1,0],[0,0,1]]\n",
    "    _m=tf.one_hot(labels,C,axis=0)\n",
    "    sess = tf.Session()\n",
    "    _hot=sess.run(_m)\n",
    "    sess.close()\n",
    "    return _hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting the corresponding Train Labels into their One Hot Matrix\n",
    "Y_train_=one_hot_matrix(train_label,46).reshape(46,78200)\n",
    "Y_test_=one_hot_matrix(test_label,46).reshape(46,13800)\n",
    "Y_train=Y_train_.T\n",
    "Y_test=Y_test_.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all the dataset is then saved into its corressponding numpy file\n",
    "np.save(\"X_train\",X_train)\n",
    "np.save(\"Y_train\",Y_train)\n",
    "np.save(\"X_test\",X_test)\n",
    "np.save(\"Y_test\",Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the saved data is loaded\n",
    "X_train=np.load(\"X_train.npy\")\n",
    "Y_train=np.load(\"Y_train.npy\")\n",
    "X_test=np.load(\"X_test.npy\")\n",
    "Y_test=np.load(\"Y_test.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train Shape =  (78200, 32, 32, 3)\n",
      "Y_train Shape =  (78200, 46)\n",
      "X_test Shape =  (13800, 32, 32, 3)\n",
      "Y_test Shape =  (13800, 46)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train Shape = \",np.shape(X_train))\n",
    "print(\"Y_train Shape = \",np.shape(Y_train))\n",
    "print(\"X_test Shape = \",np.shape(X_test))\n",
    "print(\"Y_test Shape = \",np.shape(Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DESIN OF THE CONVOLUTIONAL NETWORK LAYERS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](convolution.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Convolutional Neural Network\n",
    "def Devnagri(input_shape):\n",
    "    X_input=Input(input_shape)\n",
    "    X=Conv2D(6,(5,5),strides=(1,1),name='conv0')(X_input)    \n",
    "    X=BatchNormalization(axis=3,name='bn0')(X) # it reduces the covariate shift thus centering the dataset and reducing the computational load\n",
    "    X=Activation('relu')(X) \n",
    "    X = MaxPooling2D((2, 2),strides=(2,2), name='max_pool_1')(X)\n",
    "    X=Conv2D(16,(5,5),strides=(1,1),name='conv1')(X)\n",
    "    X=BatchNormalization(axis=3,name='bn1')(X)\n",
    "    X=Activation('relu')(X)\n",
    "    X = MaxPooling2D((2, 2),strides=(2,2), name='max_pool_2')(X)\n",
    "    X = Flatten()(X)\n",
    "    X = Dense(46, activation='softmax', name='fc1',kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    model = Model(inputs = X_input, outputs = X, name='Devnagri')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMPUTING THE DEVNAGRI MODEL WITH ADAM OPTIMIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "devnagri_model=Devnagri(X_train.shape[1:])\n",
    "devnagri_model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "devnagri_model.fit(x = X_train, y = Y_train, epochs = 20,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the model learned\n",
    "devnagri_model.save(\"devnagri_params.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the learned model\n",
    "devnagri_model = load_model(\"devnagri_params.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACCURACY ON THE TEST SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13800/13800 [==============================] - 17s 1ms/step\n",
      "Loss = 0.13500062845882987\n",
      "Test Accuracy = 0.9660869565217391\n"
     ]
    }
   ],
   "source": [
    "preds = devnagri_model.evaluate(X_test, Y_test)\n",
    "print (\"Loss = \" + str(preds[0]))\n",
    "print (\"Test Accuracy = \" + str(preds[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 32, 32, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv0 (Conv2D)               (None, 28, 28, 6)         456       \n",
      "_________________________________________________________________\n",
      "bn0 (BatchNormalization)     (None, 28, 28, 6)         24        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 28, 28, 6)         0         \n",
      "_________________________________________________________________\n",
      "max_pool_1 (MaxPooling2D)    (None, 14, 14, 6)         0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv2D)               (None, 10, 10, 16)        2416      \n",
      "_________________________________________________________________\n",
      "bn1 (BatchNormalization)     (None, 10, 10, 16)        64        \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 10, 10, 16)        0         \n",
      "_________________________________________________________________\n",
      "max_pool_2 (MaxPooling2D)    (None, 5, 5, 16)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 46)                18446     \n",
      "=================================================================\n",
      "Total params: 21,406\n",
      "Trainable params: 21,362\n",
      "Non-trainable params: 44\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#model summary\n",
    "devnagri_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VERIFY THE CLASSIFIER "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.09726339e-33 3.94538637e-38 1.67574948e-33 9.00184855e-22\n",
      "  3.99659857e-17 1.37144449e-15 5.05174751e-18 3.30807054e-23\n",
      "  0.00000000e+00 1.99991879e-32 3.23042681e-09 1.55283230e-19\n",
      "  1.48386822e-17 4.58221489e-15 2.39896988e-35 8.52646395e-27\n",
      "  1.67254693e-29 6.99944088e-17 3.66115748e-22 1.12013974e-29\n",
      "  3.35689021e-20 1.34109784e-28 1.82190415e-26 6.76518200e-37\n",
      "  0.00000000e+00 1.98500786e-23 1.01015197e-16 2.59925048e-30\n",
      "  6.80600977e-20 2.70462856e-30 2.63097321e-30 3.97689261e-30\n",
      "  1.56801807e-14 7.19921359e-31 3.94004792e-30 1.47305921e-36\n",
      "  2.23820162e-13 1.08937690e-22 1.70823201e-20 1.56700491e-21\n",
      "  1.46890596e-22 3.35217105e-26 7.69993533e-19 6.05939719e-20\n",
      "  1.00000000e+00 5.58290510e-19]]\n",
      "44\n",
      "The Image loaded is =  8\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1b85ff9a7f0>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#ENTER THE NAME OF THE IMAGE IN THE FILE ADDRESS TO DISPLAY THE RESULT\n",
    "img = image.load_img('8.png', target_size=(32, 32))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)\n",
    "imshow(img)\n",
    "Prediction={0:'ka',1:'kha',2:'ga',3:'gha',4:'kna',5:'cha',6:'chha',7:'ja',8:'jha',9:'yna',10:'taamatar',11:'thaa',12:'daa',13:'dhaa',14:'adna',15:'tabala',16:'tha',17:'da',18:'dha',19:'na',20:'pa',21:'pha',22:'ba',23:'bha',24:'ma',25:'yaw',26:'ra',27:'la',28:'waw',29:'motosaw',30:'petchiryakha',31:'patalosaw',32:'ha',33:'chhya',34:'tra',35:'gya',36:'0',37:'1',38:'2',39:'3',40:'4',41:'5',42:'6',43:'7',44:'8',45:'9'}\n",
    "print((devnagri_model.predict(x)))\n",
    "print(np.argmax(devnagri_model.predict(x)))\n",
    "a=np.argmax(devnagri_model.predict(x))\n",
    "print('The Image loaded is = ',Prediction[a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
